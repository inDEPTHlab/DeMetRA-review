# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "openpyxl==3.1.5",
# ]
# ///

import marimo

__generated_with = "0.15.0"
app = marimo.App(width="medium")


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    # DeMetRA - literature review

    This the preprocessing pipeline for the metadata included in the DeMetRA review. Inputs to the pipeline are: 

    - The `MPS_review_systematic_DATE.xlsx` file, which contains the metadata manually extracted by Isabel.
    - The `Bibliography_<date>.txt` file, wich is additionally parsed to supplement data cleaning add additional information about the included publications (i.e. publication date, abstracts, keywords). <br>This file was generated by exporting the .ris or .enl files as a RefMan (RIS) .txt file (using Endnote).
    """
    )
    return


@app.cell
def _():
    import marimo as mo

    import pandas as pd
    import numpy as np

    from data_preprocessing_helpers import read_sys_review, read_bibliography, inspect_variable_levels, summarize_dimension_reduction_strategies, count_categories_per_phenotype, replace_multiples, clean_n_CpGs, coerce_to_numeric, aggregate_values

    assets_directory = './assets/'

    sys_review_file = 'MPS_review_systematic_2025-08-14.xlsx' # 'MPS_review_systematic_2025-02-14.xlsx'
    bibliograp_file = 'Bibliography_2025-07-25.txt'

    return (
        aggregate_values,
        assets_directory,
        bibliograp_file,
        clean_n_CpGs,
        coerce_to_numeric,
        count_categories_per_phenotype,
        inspect_variable_levels,
        mo,
        read_bibliography,
        read_sys_review,
        replace_multiples,
        summarize_dimension_reduction_strategies,
        sys_review_file,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    In the systematic review file created by Isabel, there are 3 sheets:

    1. The main list of MPSs / papers included in the review
    2. The base sample references papers for summary statistics 
    3. The base sample references papers for validated algorithms
    """
    )
    return


@app.cell
def _(
    assets_directory,
    bibliograp_file,
    mo,
    read_bibliography,
    read_sys_review,
    sys_review_file,
):
    lit_raw, lit_base_raw = read_sys_review(assets_directory, sys_review_file)

    bib = read_bibliography(assets_directory, bibliograp_file, title_list = lit_raw.Title.unique())

    rev_data = {"Main": lit_raw, "Base": lit_base_raw}

    mo.ui.tabs(rev_data | {"Bibliography": bib})
    return bib, lit_base_raw, lit_raw, rev_data


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""## STEP 1 - Data cleaning""")
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Data inspection

    To guide manual cleaning of the excel file, I print value counts for the main columns of interest.

    - Sample information: `'Tissue'`, `'Array'`, `'Ancestry'`, `'Developmental_period'`
    - Dimensionality reduction strategies: `'Including_CpGs_*'`
    - Weight estimation strategies: `'Determining_weights_*'`
    - Internal and external validatation: `'Train_test'`/ `'Train_validate'` & `'Independent_validation'` / `'Independent_test'`
    - Performance: `'Reflect_phenotype'`
    """
    )
    return


@app.cell
def _(inspect_variable_levels, mo, rev_data):
    # Inspect values 
    # Returns value count for each variable and its associated "Multiple" column
    # NOTE: this is to guide manual cleaning of the excel file!

    vars_to_inspect = ['Tissue', 'Array', 'Ancestry', 'Developmental_period']

    mo.ui.tabs({v: inspect_variable_levels(var = v, **rev_data, sort_index = False) for v in vars_to_inspect})
    return


@app.cell(hide_code=True)
def _(rev_data, summarize_dimension_reduction_strategies):
    summarize_dimension_reduction_strategies(**rev_data)
    return


@app.cell
def _(inspect_variable_levels, rev_data):
    # Summarize weight estimation strategies
    inspect_variable_levels(var = "Determining_weights_1", **rev_data)
    return


@app.cell
def _(inspect_variable_levels, rev_data):
    # Summarize internal validation strategies
    inspect_variable_levels(var = "Train_test", var_base = "Train_validate", **rev_data)
    return


@app.cell
def _(inspect_variable_levels, rev_data):
    # Summarize external validation strategies
    inspect_variable_levels(var = "Independent_validation", var_base = "Independent_test", **rev_data)
    return


@app.cell
def _(inspect_variable_levels, rev_data):
    # Summarize performance 
    inspect_variable_levels(var = "Reflect_phenotype", **rev_data)
    return


@app.cell
def _(inspect_variable_levels, rev_data):
    inspect_variable_levels(var = "Sample_overlap_target_base", **rev_data)
    return


@app.cell
def _(count_categories_per_phenotype, rev_data):
    # Check that no phenotypes are assigned to multiple categories 
    count_categories_per_phenotype(**rev_data)
    return


@app.cell
def _():
    # Covariates
    # Comparison, 'Missing_value_note', 'Link'
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        """
    ### Data cleaning 

    1. Extract and remove the **`"*_multiple"`** `Tissue`, `Array`, and `Ancestry` columns so that only one column is kept in the dataset.
       I do this for both the main list and the base samples.

    3. Clean the **`number of CpGs`** column by replacing `NA` with base sample sizes and setting "Not reported" to NA.
       I also remove 2 scores that have `Number of CpGs == 0` (because they did not identify a solution).

    5. Clean the **"Sample size"** columns so that they are all numeric.
    """
    )
    return


@app.cell
def _(
    bib,
    clean_n_CpGs,
    coerce_to_numeric,
    lit_base_raw,
    lit_raw,
    replace_multiples,
):
    # Clean *_multiple variables ------------------------------------------------------------
    lit_main = replace_multiples(lit_raw)
    lit_base = replace_multiples(lit_base_raw)

    # Clean Number of CpGs ------------------------------------------------------------------

    lit_base = clean_n_CpGs(lit_base)
    # Note: if this is missing, I attempt retrieving the value form base whenever possible, 
    # but these are assumed to match (checked later)
    lit_main = clean_n_CpGs(lit_main, replace_with_df = lit_base)

    # Clean Sample size columns -------------------------------------------------------------

    lit_main = coerce_to_numeric(lit_main, 'Sample_size_total', 'Sample size')
    lit_main = coerce_to_numeric(lit_main, 'Sample_size_case', 'n Cases')
    lit_main = coerce_to_numeric(lit_main, 'Sample_size_control', 'n Controls')

    lit_base = coerce_to_numeric(lit_base, 'Sample_size_total', 'Sample size')
    lit_base = coerce_to_numeric(lit_base, 'Sample_size_case', 'n Cases')
    lit_base = coerce_to_numeric(lit_base, 'Sample_size_control', 'n Controls')

    # Clean Dinesionality reduction ---------------------------------------------------------

    lit_main['Including_CpGs_1'] = lit_main['Including_CpGs_1'].fillna('None')
    lit_base['Including_CpGs_1'] = lit_base['Including_CpGs_1'].fillna('None')

    # Add bibliography info -----------------------------------------------------------------

    lit_main = lit_main.rename(columns={'Author': 'Author_dirty', 
                                        'Journal': 'Journal_dirty',
                                        'DOI': 'DOI_dirty'}).merge(bib, on='Title', how='left', suffixes=['', '_BIB'])

    # lit_main.loc[lit_main.Journal != lit_main.Journal_BIB, ['Journal', 'Journal_BIB','Title']]
    # lit_main.loc[lit_main.Author != lit_main.Author_BIB, ['Author', 'Author_BIB','Title']]
    # lit_main.loc[lit_main.DOI != lit_main.DOI_BIB, ['DOI', 'DOI_BIB','Title']]
    # lit_main.loc[lit_main.Year != lit_main.Year_BIB.astype(int), ['Year', 'Year_BIB','Title']]
    return lit_base, lit_main


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""### Rename and reorder""")
    return


@app.cell
def _(lit_base, lit_main):
    # lit_main.What_is_available.value_counts(dropna=False)

    rename_based_on = {
        "Only phenotype": "Raw individual-level data",
        "Validated MPS algorithm": "Pre-established MPS",
        "EWAS summary statistics": "Published summary statistics (semi-supervised)"
    }

    lit_main["Based on"] = lit_main["What_is_available"].map(rename_based_on) # .fillna(df["What_is_available"]) # no NA values here

    rename_cols_dict = {'Sample_type': 'Sample type', 
                        'Developmental_period': 'Developmental period', 
                        'Including_CpGs_1' : 'Dimension reduction (1)', 
                        'Including_CpGs_2': 'Dimension reduction (2)', 
                        'Including_CpGs_3': 'Dimension reduction (3)',  
                        'Including_CpGs_4': 'Dimension reduction (4)',  
                        'Including_CpGs_5': 'Dimension reduction (5)', 
                        'Determining_weights_1': 'Weights estimation',
                        'Train_test': 'Internal validation',
                        'Train_validate': 'Internal validation',
                        'Independent_validation': 'External validation',
                        'Independent_test': 'External validation',
                        'Reflect_phenotype': 'Performance',
                        'Type': 'Publication type'}

    mps_table = lit_main.rename(columns=rename_cols_dict)
    base_table = lit_base.rename(columns=rename_cols_dict)

    return base_table, mps_table


@app.cell(hide_code=True)
def _(mo):
    mo.md(
        r"""
    ### Publications table 
    Create a  table grouped by "paper" rather than by "MPS"
    """
    )
    return


@app.cell
def _(aggregate_values, mps_table):
    pub_table = mps_table.groupby('Title').agg(aggregate_values).reset_index()
    n_MPS = mps_table.Title.value_counts()
    for _t in n_MPS.index:
        pub_table.loc[pub_table.Title == _t, 'n MPSs'] = int(n_MPS[_t])
    return (pub_table,)


@app.cell
def _(mps_base_matched, pub_table):
    pub_table['Dimension reduction (1)'][pub_table['Dimension reduction (1)'].str.contains('Multiple (', regex=False
                                                                                          )].value_counts()

    mps_base_matched['Ancestry [development]'] = mps_base_matched['Ancestry [development]'].replace({
        'Multiple (Association DNAm phenotype | P-value | ?, None)': '[Association DNAm phenotype | P-value | ?; None]',
        'Multiple (Association DNAm phenotype | P-value | bonferroni < .05, Association DNAm phenotype | P-value | FDR < .05)': 'Association DNAm phenotype | P-value | [bonferroni < .05; FDR < .05]',
        'Multiple (Association DNAm phenotype | P-value | top-ranking 100, Association DNAm phenotype | P-value | top-ranking 1000, Association DNAm phenotype | P-value | top-ranking 10000, Association DNAm phenotype | P-value | top-ranking 20, Association DNAm phenotype | P-value | top-ranking 50, Association DNAm phenotype | P-value | top-ranking 500, Association DNAm phenotype | P-value | top-ranking 5000)': '',
        'Multiple (Association DNAm phenotype | P-value | p < .1, Association DNAm phenotype | P-value | p < .01, Association DNAm phenotype | P-value | p < .001, Association DNAm phenotype | P-value | p < 1e-4, Association DNAm phenotype | P-value | p < 1e-5)': '',
        'Multiple (Association DNAm phenotype | P-value | FDR < .05, None)': '',
        'Multiple (None, Biological relevance | Functional annotation | Included in EpiSign)': '',
        'Multiple (Association DNAm phenotype | P-value | p < .05, Association DNAm phenotype | P-value | adjusted p < .05)': '',
        'Multiple (Association DNAm phenotype | P-value | top-ranking ?, Association DNAm phenotype | P-value | top-ranking 200)': '',
        'Multiple (Association DNAm phenotype | P-value | top-ranking 1000, Association DNAm phenotype | P-value | top-ranking 900-1000)': '',
        'Multiple (Association DNAm phenotype | Methylation change | > 15%, Association DNAm phenotype | Methylation change | > 5%, Association DNAm phenotype | Methylation change | > 20%, Association DNAm phenotype | Methylation change | > 10%, Association DNAm phenotype | Methylation change | > 25%)': '',
        'Multiple (Association DNAm phenotype | P-value | top-ranking 1000, None)': '',
        'Multiple (Association DNAm phenotype | P-value | p < 1e-5, None)': '',
        'Multiple (None, Association DNAm phenotype | P-value | Multiple cut-offs [thresholding])': '',
        'Multiple (None, Biological relevance | Mean methylation | Zero variance)': ''})
    return


@app.cell
def _(mo):
    mo.md(
        r"""
    ### Mearge target and base samples

    <ins>Note</ins>:&emsp;base -> "development dataset"; <br>&emsp;&emsp;&emsp; target -> "application dataset"
    """
    )
    return


@app.cell
def _(base_table, mps_table):
    targ_only = mps_table.loc[mps_table['Based on'] != 'Raw individual-level data',]
    print(targ_only.shape, base_table.shape)
    return (targ_only,)


@app.cell
def _(targ_only):
    multi_base_id = targ_only.loc[targ_only.Identifier_base == 'Multiple',]
    multi_base_pubs = multi_base_id.Title.unique()
    print(f"{multi_base_id.shape[0]} MPSs (from {len(multi_base_pubs)} publications) have multiple base samples.")
    print(multi_base_id['Based on'].value_counts())
    print("\n", targ_only['Multiple_identifier_base'].value_counts())
    return


@app.cell
def _(base_table, targ_only):
    # For Multiple base IDs linked to the same target MPS: split identifiers into rows
    targ_only.loc[:, "Identifier_base_expanded"] = targ_only.apply(
        lambda row: [row["Identifier_base"]] if row["Identifier_base"] != "Multiple" 
        else row["Multiple_identifier_base"].split("; "), axis=1
    )

    # And unpack multiples
    targ_expanded = targ_only.explode("Identifier_base_expanded")

    # For base IDs with multiple rows (i.e., phenotypes), append phenotype info to the IDs to they are truly unique
    base_id_counts = base_table.groupby("Identifier")["Identifier"].transform("count")
    base_id_mask = base_id_counts > 1
    targ_id_mask = targ_expanded["Identifier_base_expanded"].isin(set(base_table.loc[base_id_mask, "Identifier"]))

    # If an Identifier appears more than once â†’ append Phenotype
    base_table.loc[:, "Identifier_base_expanded"] = base_table["Identifier"]
    base_table.loc[base_id_mask, "Identifier_base_expanded"] = (base_table["Identifier"] + base_table["Phenotype"])

    targ_expanded.loc[targ_id_mask, "Identifier_base_expanded"] = (
        targ_expanded.loc[targ_id_mask, "Identifier_base_expanded"] + targ_expanded.loc[targ_id_mask, "Phenotype"])

    # In case the same base paper is used for summary stats and validated score, add info to IDs to ensure they are still unique
    base_table_use = ['ss' if use == 'Summary statistics' else 'va' for use in base_table["Base use"]]
    targ_table_use = ['ss' if use == 'Published summary statistics (semi-supervised)' else 'va' for use in targ_expanded['Based on']]

    base_table.loc[:, "Identifier_base_expanded"] = base_table["Identifier_base_expanded"] + base_table_use
    targ_expanded.loc[:, "Identifier_base_expanded"] = targ_expanded["Identifier_base_expanded"] + targ_table_use
    return (targ_expanded,)


@app.cell
def _(base_table):
    count_duplicate_base_ids = base_table['Identifier_base_expanded'].value_counts()

    duplicate_base_ids = list(count_duplicate_base_ids.index[count_duplicate_base_ids > 1])
    base_table.loc[base_table['Identifier_base_expanded'].isin(duplicate_base_ids), :]

    # TMP: I pick duplicate with more info, drop the others
    duplicate_idx = [155, 156]
    base_table_tmp = base_table[~base_table.index.isin(duplicate_idx)]
    return (base_table_tmp,)


@app.cell
def _(base_table_tmp, targ_expanded):
    # Check mismatches
    targ_ids = set(targ_expanded['Identifier_base_expanded'])
    base_ids = set(base_table_tmp['Identifier_base_expanded'])

    common_ids = targ_ids & base_ids
    unique_to_base = base_ids - targ_ids
    unique_to_targ = targ_ids - base_ids

    print(len(common_ids), 'matching ids\n')

    print(f"{len(unique_to_base)} / {len(base_ids)} unique to base")
    if (len(unique_to_base) > 0): [print('\t', p) for p in unique_to_base]

    print(f"{len(unique_to_targ)} / {len(targ_ids)} unique to targ")
    if (len(unique_to_targ) > 0): [print('\t', p) for p in unique_to_targ]

    # Remove "Unclear" base links
    print('\nCleaning...\n')
    targ_expanded_clean = targ_expanded.loc[targ_expanded["Identifier_base_expanded"] != "Unclearva", :]
    targ_ids = set(targ_expanded_clean['Identifier_base_expanded'])
    unique_to_targ = targ_ids - base_ids
    print(f"{len(unique_to_targ)} / {len(targ_ids)} unique to targ")
    if (len(unique_to_targ) > 0): [print('\t', p) for p in unique_to_targ]

    # Inspect 
    # mo.ui.tabs({"Main": targ_expanded.loc[targ_expanded['Identifier_base_expanded'].isin(unique_to_targ), :], 
    #             "Base": d_base.loc[d_base['Identifier_base_expanded'].isin(unique_to_base), :], 
    #            })
    return (targ_expanded_clean,)


@app.cell
def _(aggregate_values, base_table_tmp, targ_expanded_clean):
    # Now I can merge with base 
    targ_base_merge = targ_expanded_clean.merge(base_table_tmp, on="Identifier_base_expanded", how="inner", # "left", 
                                            suffixes=[' [application]', ' [development]'])

    # Group back by original targ row
    targ_base_aggre = targ_base_merge.groupby(targ_expanded_clean.index).agg(aggregate_values).reset_index(drop=True)

    print(targ_base_aggre.shape, sorted(targ_base_aggre.columns))
    targ_base_aggre
    # d_targ_base.to_csv(f'{assets_directory}MPS_base_target_cleaned.csv', index=False)
    return (targ_base_aggre,)


@app.cell
def _(targ_base_aggre):
    mps_base_matched = targ_base_aggre.copy()
    mps_base_matched['Array [development]'] = mps_base_matched['Array [development]'].replace({
        'Multiple (Multiple (450K, EPICv1), 450K, EPICv1)': 'Multiple (450K, EPICv1)', 
        'Multiple (EPICv1, 450K, PCR)': 'Multiple (450K, EPICv1, PCR)'})

    mps_base_matched['Developmental period [development]'] = mps_base_matched['Developmental period [development]'].replace({
        'Multiple (Childhood, Birth, Childhood and adolescence, Adolescence)': 'Multiple (Birth to Adolescence)'})

    mps_base_matched['Ancestry [development]'] = mps_base_matched['Ancestry [development]'].replace({
        'Multiple (Mixed, Australian, European)': 'Mixed', 
        'Multiple (Mixed, White)': 'Mixed',
        'Multiple (European, Mixed)': 'Mixed'})
    return (mps_base_matched,)


@app.cell
def _(mo, mps_base_matched):
    def detect_mismatch(variable, df = mps_base_matched, x=' [development]', y=' [application]'):
        df_subset = df.loc[df[f"{variable}{x}"] != df[f"{variable}{y}"], :]
        value_counts = (df_subset[f"{variable}{x}"].astype(str) +" --- "+ 
                        df_subset[f"{variable}{y}"].astype(str)).value_counts()
        return df_subset, value_counts

    vars_to_inspect_match = ['Phenotype', 'Category', 'Developmental period', 'Tissue', 'Array', 'Ancestry',
                       # Dimension reduction (n)
                       # Weights estimation
                       # Internal / external validation
                       # Performance
                      'Sample type']

    mo.ui.tabs({v: detect_mismatch(v) for v in vars_to_inspect_match})
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""###Save cleaned files""")
    return


@app.cell
def _(mo, mps_base_matched, mps_table, pub_table):
    # Subset and reorder
    mps_table_clean = mps_table[['Phenotype', 'Category', 'Author', 'Year', 'Title', 'DOI', 
                                 'n CpGs', 'Based on', 'Sample size', 'n Cases', 'n Controls', 'Sample type', 
                                 'Developmental period', 'Tissue', 'Array', 'Ancestry', 
                                 'Publication type', 'Journal'] + 
                                ['Keywords', 'Abstract', 'Author_list', 'Date'] + 
                                [f'Dimension reduction ({i})' for i in range(1, 6)] +
                                ['Weights estimation', 'Internal validation', 'External validation', 'Performance',
                                 'Comparison', 'Missing_value_note', 'Covariates']]

    pub_table_clean = pub_table[['Author', 'Year', 'Title', 'DOI', 'n MPSs', 'Phenotype', 'Category', 
                                 'n CpGs', 'Based on', 'Sample size', 'n Cases', 'n Controls', 'Sample type', 
                                 'Developmental period', 'Tissue', 'Array', 'Ancestry', 
                                 'Publication type', 'Journal'] + 
                                ['Keywords', 'Abstract', 'Author_list', 'Date'] + 
                                [f'Dimension reduction ({i})' for i in range(1, 6)] +
                                ['Weights estimation', 'Internal validation', 'External validation', 'Performance',
                                 'Comparison', 'Missing_value_note', 'Covariates']
                                ].rename(columns={'Phenotype': 'Phenotype(s)'})

    mps_base_matched_clean = mps_base_matched[[
        'Phenotype [application]', 'Phenotype [development]', 
        'Category [application]', 'Category [development]', 
        'Author', 'Year', 'Title', 
        'n CpGs [application]', 'n CpGs [development]',
        'Based on', 
        'Sample size [application]', 'Sample size [development]', 
        'n Cases [application]', 'n Cases [development]', 
        'n Controls [application]', 'n Controls [development]', 
        'Sample type [application]', 'Sample type [development]', 
        'Developmental period [application]', 'Developmental period [development]', 
        'Tissue [application]', 'Tissue [development]', 
        'Array [application]', 'Array [development]', 
        'Ancestry [application]', 'Ancestry [development]', 
        'Dimension reduction (1) [application]', 'Dimension reduction (1) [development]', 
        'Dimension reduction (2) [application]', 'Dimension reduction (2) [development]', 
        'Dimension reduction (3) [application]', 'Dimension reduction (3) [development]', 
        'Dimension reduction (4) [application]', 'Dimension reduction (4) [development]', 
        'Dimension reduction (5) [application]', 'Dimension reduction (5) [development]', 
        'Weights estimation [application]', 'Weights estimation [development]', 
        'Internal validation [application]', 'Internal validation [development]', 
        'External validation [application]', 'External validation [development]',
        'Performance [application]', 'Performance [development]', 
        'Comparison [application]', 'Comparison [development]', 
        'Missing_value_note [application]', 'Missing_value_note [development]', 
        'Covariates [application]', 'Covariates [development]',
        'Sample_overlap_target_base [application]', 'Sample_overlap_target_base [development]']]

    mo.ui.tabs({"Pubs table": pub_table_clean, "MPS table": mps_table_clean, "MPS table (base-matched)": mps_base_matched_clean})
    return mps_base_matched_clean, mps_table_clean, pub_table_clean


@app.cell
def _(
    assets_directory,
    mps_base_matched_clean,
    mps_table_clean,
    pub_table_clean,
):
    mps_table_clean.to_csv(f'{assets_directory}MPS_table_cleaned.csv', index=False)
    pub_table_clean.to_csv(f'{assets_directory}Publication_table_cleaned.csv', index=False)

    mps_base_matched_clean.to_csv(f'{assets_directory}MPS_base_matched_cleaned.csv', index=False)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""### Publication network graph""")
    return


@app.cell
def _():
    import networkx as nx
    import pickle
    return nx, pickle


@app.cell(hide_code=True)
def _(mo):

    # Make cell wait for my input
    run_button = mo.ui.run_button()
    run_button

    return (run_button,)


@app.cell
def _(assets_directory, mo, nx, pickle, pub_table_clean, run_button):
    mo.stop(not run_button.value, mo.md("Click ðŸ‘† to run this cell"))

    G = nx.Graph()
    for pub in pub_table_clean.Title:
        title = f"Paper/{pub.replace(':', ' ')}"
        authors = pub_table_clean.loc[pub_table_clean.Title == pub, 'Author_list'].iloc[0]
        for author in authors:
            author = f"Author/{author.replace('. ', '.')}"
            G.add_edge(author, title)

    # Estimate optimal node positions ----------------------------------------------------
    def estimate_node_positions(G):
        # pos = nx.spring_layout(G, seed = 3108, k = 1000)
        pos = nx.fruchterman_reingold_layout(G, seed = 3108, k = 0.05)

        # Assign 'pos' attribute to the nodes in the graph
        for node in G.nodes:
            G.nodes[node]['pos'] = pos[node]

        return G

    pub_network = estimate_node_positions(G)

    with open(f'{assets_directory}/Publications_network.pkl', 'wb') as _file:
        pickle.dump(pub_network, _file)

    mo.md("Done! ðŸŽ‰")
    return


@app.cell
def _():
    # with open(f'{assets_directory}/Publications_network.pkl', 'rb') as _file:
    #     pub_network_check = pickle.load(_file)
    return


if __name__ == "__main__":
    app.run()
